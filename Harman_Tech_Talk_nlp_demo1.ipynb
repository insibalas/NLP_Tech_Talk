{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05cbb623",
   "metadata": {},
   "source": [
    "### Example - Translator - English to Tamil , Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d575f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil Translation: எப்படி இருக்கிறீர்கள்?\n",
      "Hindi Translation: आप कैसे हैं?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def translate_english_to_tamil(text):\n",
    "    try:\n",
    "        url = \"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=ta&dt=t&q=\" + text\n",
    "        response = requests.get(url)\n",
    "        result = response.json()\n",
    "        return result[0][0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def translate_english_to_hindi(text):\n",
    "    try:\n",
    "        url = \"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=hi&dt=t&q=\" + text\n",
    "        response = requests.get(url)\n",
    "        result = response.json()\n",
    "        return result[0][0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "english_text = \"How are you?\"\n",
    "tamil_translation = translate_english_to_tamil(english_text)\n",
    "hindi_translation = translate_english_to_hindi(english_text)\n",
    "print(\"Tamil Translation:\", tamil_translation)\n",
    "print(\"Hindi Translation:\", hindi_translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eadbf9",
   "metadata": {},
   "source": [
    "The program uses the Google Translate API to translate text from English to Tamil and Hindi. Here's a step-by-step explanation of how it works:\n",
    "\n",
    "### 1. **Imports the Required Library**\n",
    "   - The program imports the `requests` library, which allows it to send HTTP requests to the Google Translate API.\n",
    "\n",
    "### 2. **Function Definitions**\n",
    "   - **`translate_english_to_tamil(text)`**:\n",
    "     - This function translates English text to Tamil.\n",
    "   - **`translate_english_to_hindi(text)`**:\n",
    "     - This function translates English text to Hindi.\n",
    "\n",
    "### 3. **How Each Function Works**\n",
    "   - **URL Construction**:\n",
    "     - The URL used in each function is constructed based on the Google Translate API endpoint.\n",
    "     - For Tamil translation:\n",
    "       ```python\n",
    "       url = \"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=ta&dt=t&q=\" + text\n",
    "       ```\n",
    "       - `client=gtx`: Specifies the client type.\n",
    "       - `sl=en`: Source language is English (`en`).\n",
    "       - `tl=ta`: Target language is Tamil (`ta`).\n",
    "       - `dt=t`: Specifies the data type (translation).\n",
    "       - `q=`: Followed by the text to be translated.\n",
    "\n",
    "     - For Hindi translation, the target language (`tl`) is changed to Hindi (`hi`).\n",
    "\n",
    "   - **Sending a Request**:\n",
    "     - The program sends a GET request to the constructed URL using `requests.get(url)`.\n",
    "\n",
    "   - **Parsing the Response**:\n",
    "     - The response from the API is in JSON format.\n",
    "     - The program extracts the translated text using `response.json()` and then accesses the specific part of the JSON that contains the translation:\n",
    "       ```python\n",
    "       result[0][0][0]\n",
    "       ```\n",
    "       - This nested list structure holds the translated text in the first element.\n",
    "\n",
    "   - **Handling Exceptions**:\n",
    "     - The `try-except` block catches any errors that might occur during the request or parsing process and prints an error message.\n",
    "\n",
    "### 4. **Example Usage**\n",
    "   - The `english_text` variable is set to `\"siva\"`.\n",
    "   - The program calls both translation functions:\n",
    "     - `translate_english_to_tamil(english_text)` translates `\"siva\"` to Tamil.\n",
    "     - `translate_english_to_hindi(english_text)` translates `\"siva\"` to Hindi.\n",
    "\n",
    "### 5. **Output**\n",
    "   - The program prints the translations:\n",
    "     ```plaintext\n",
    "     Tamil Translation: சிவா\n",
    "     Hindi Translation: शिवा\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "- The program sends a request to the Google Translate API for each translation.\n",
    "- It retrieves and prints the translated text in Tamil and Hindi by accessing the relevant parts of the JSON response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e556e",
   "metadata": {},
   "source": [
    "Yes, Natural Language Processing (NLP) plays a significant role in how the translation process works behind the scenes, even though the specific implementation of NLP is abstracted away from the user in this program.\n",
    "\n",
    "### How NLP is Involved in This Process\n",
    "\n",
    "1. **Language Detection**:\n",
    "   - Although the source language (`sl=en`) is explicitly specified in this program, in many cases, translation systems automatically detect the source language. NLP techniques are used to analyze the input text and determine its language based on patterns, vocabulary, and syntax.\n",
    "\n",
    "2. **Tokenization**:\n",
    "   - NLP breaks down the input text into smaller units like words or phrases (tokens). This is crucial for understanding the context and meaning of each part of the text during translation.\n",
    "\n",
    "3. **Syntax and Grammar Analysis**:\n",
    "   - NLP involves parsing the structure of sentences to understand the relationships between words. This helps in correctly translating phrases and sentences while maintaining grammatical correctness in the target language.\n",
    "\n",
    "4. **Context Understanding**:\n",
    "   - Advanced NLP models, such as those used by Google Translate, understand the context of words and phrases. This allows them to choose the most appropriate translation for words that may have multiple meanings depending on the context.\n",
    "\n",
    "5. **Translation Memory and Statistical Models**:\n",
    "   - NLP systems often use a vast database of previous translations to make predictions. They may use statistical models or machine learning techniques to generate translations that are contextually and grammatically appropriate.\n",
    "\n",
    "6. **Machine Learning and Neural Networks**:\n",
    "   - Modern NLP heavily relies on deep learning models, particularly neural networks, which are trained on massive amounts of multilingual data. These models can understand and generate human-like text in different languages, enabling more accurate translations.\n",
    "\n",
    "7. **Handling Ambiguity and Idiomatic Expressions**:\n",
    "   - NLP techniques help in dealing with ambiguous terms, idiomatic expressions, and cultural nuances that can be tricky to translate directly. The system uses context and previous examples to provide more accurate translations.\n",
    "\n",
    "### In Summary\n",
    "The program  interfaces with a simple API call, the underlying system that powers Google Translate uses a sophisticated combination of NLP techniques. These techniques ensure that the translations are accurate, contextually appropriate, and grammatically correct. The heavy lifting in terms of NLP happens on the server side, and the program interacts with these advanced NLP processes through the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980096b4",
   "metadata": {},
   "source": [
    "### Example - Corpus , Tokenize the corpus into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51234e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
      "['She', 'sells', 'sea', 'shells', 'by', 'the', 'seashore.']\n",
      "['Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers.']\n",
      "['How', 'much', 'wood', 'would', 'a', 'woodchuck', 'chuck', 'if', 'a', 'woodchuck', 'could', 'chuck', 'wood?']\n",
      "['Sally', 'sells', 'seashells', 'by', 'the', 'seashore.']\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Define the small corpus\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"She sells sea shells by the seashore.\",\n",
    "    \"Peter Piper picked a peck of pickled peppers.\",\n",
    "    \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "    \"Sally sells seashells by the seashore.\"\n",
    "]\n",
    "# Tokenize the corpus into words\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "for sentence_tokens in tokenized_corpus:\n",
    "    print(sentence_tokens)\n",
    "# Tokenize the corpus into words and convert to lowercase\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    unique_words.update(tokens)\n",
    "\n",
    "# Print the unique words\n",
    "#print(\"Unique words in the corpus:\")\n",
    "#for word in unique_words:\n",
    "#   print(word)\n",
    "print(len(unique_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276ccd6",
   "metadata": {},
   "source": [
    "### Example - The  n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1330e2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:\n",
      "[('The',), ('quick',), ('brown',), ('fox',), ('jumps',), ('over',), ('the',), ('lazy',), ('dog.',), ('She',), ('sells',), ('sea',), ('shells',), ('by',), ('the',), ('seashore.',), ('Peter',), ('Piper',), ('picked',), ('a',), ('peck',), ('of',), ('pickled',), ('peppers.',), ('How',), ('much',), ('wood',), ('would',), ('a',), ('woodchuck',), ('chuck',), ('if',), ('a',), ('woodchuck',), ('could',), ('chuck',), ('wood?',), ('Sally',), ('sells',), ('seashells',), ('by',), ('the',), ('seashore.',)]\n",
      "43\n",
      "\n",
      "Bigrams:\n",
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog.'), ('She', 'sells'), ('sells', 'sea'), ('sea', 'shells'), ('shells', 'by'), ('by', 'the'), ('the', 'seashore.'), ('Peter', 'Piper'), ('Piper', 'picked'), ('picked', 'a'), ('a', 'peck'), ('peck', 'of'), ('of', 'pickled'), ('pickled', 'peppers.'), ('How', 'much'), ('much', 'wood'), ('wood', 'would'), ('would', 'a'), ('a', 'woodchuck'), ('woodchuck', 'chuck'), ('chuck', 'if'), ('if', 'a'), ('a', 'woodchuck'), ('woodchuck', 'could'), ('could', 'chuck'), ('chuck', 'wood?'), ('Sally', 'sells'), ('sells', 'seashells'), ('seashells', 'by'), ('by', 'the'), ('the', 'seashore.')]\n",
      "38\n",
      "\n",
      "Trigrams:\n",
      "[('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog.'), ('She', 'sells', 'sea'), ('sells', 'sea', 'shells'), ('sea', 'shells', 'by'), ('shells', 'by', 'the'), ('by', 'the', 'seashore.'), ('Peter', 'Piper', 'picked'), ('Piper', 'picked', 'a'), ('picked', 'a', 'peck'), ('a', 'peck', 'of'), ('peck', 'of', 'pickled'), ('of', 'pickled', 'peppers.'), ('How', 'much', 'wood'), ('much', 'wood', 'would'), ('wood', 'would', 'a'), ('would', 'a', 'woodchuck'), ('a', 'woodchuck', 'chuck'), ('woodchuck', 'chuck', 'if'), ('chuck', 'if', 'a'), ('if', 'a', 'woodchuck'), ('a', 'woodchuck', 'could'), ('woodchuck', 'could', 'chuck'), ('could', 'chuck', 'wood?'), ('Sally', 'sells', 'seashells'), ('sells', 'seashells', 'by'), ('seashells', 'by', 'the'), ('by', 'the', 'seashore.')]\n",
      "33\n",
      "<FreqDist with 35 samples and 38 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Generate unigrams, bigrams, and trigrams for each sentence in the corpus\n",
    "unigrams = [gram for sentence in tokenized_corpus for gram in ngrams(sentence, 1)]\n",
    "bigrams = [gram for sentence in tokenized_corpus for gram in ngrams(sentence, 2)]\n",
    "trigrams = [gram for sentence in tokenized_corpus for gram in ngrams(sentence, 3)]\n",
    "\n",
    "# Print the generated n-grams\n",
    "print(\"Unigrams:\")\n",
    "print(unigrams)\n",
    "print(len(unigrams))\n",
    "\n",
    "print(\"\\nBigrams:\")\n",
    "print(bigrams)\n",
    "print(len(bigrams))\n",
    "\n",
    "print(\"\\nTrigrams:\")\n",
    "print(trigrams)\n",
    "print(len(trigrams))\n",
    "# Build a bigram frequency distribution\n",
    "bigram_freq_dist = FreqDist(bigrams)\n",
    "print(bigram_freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efb5e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sally', 'sells', 'seashells', 'seashore', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c122cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salli', 'sell', 'seashel', 'seashor', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b97536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 1]\n",
      " [0 1 1 0 0]]\n",
      "['an' 'another' 'example' 'is' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\"This is an example.\", \"Another example.\"])\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5239684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
